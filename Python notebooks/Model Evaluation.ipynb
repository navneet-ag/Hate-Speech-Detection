{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myclasses.Features import Features\n",
    "from myclasses.Data import Data\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    with open(path, \"rb\") as fp:\n",
    "        clf = pickle.load(fp)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathA=\"./models/subtaska/\"\n",
    "pathB=\"./models/subtaskb/\"\n",
    "pathC=\"./models/subtaskc/\"\n",
    "\n",
    "dataA=load_model(pathA + \"/data_obj\")\n",
    "dataB=load_model(pathB + \"/data_obj\")\n",
    "dataC=load_model(pathC + \"/data_obj\")\n",
    "\n",
    "X_TestA=load_model(pathA + \"/testX\")\n",
    "Y_TestA=load_model(pathA + \"/testY\")\n",
    "\n",
    "X_TestB=load_model(pathB + \"/testX\")\n",
    "Y_TestB=load_model(pathB + \"/testY\")\n",
    "\n",
    "X_TestC=load_model(pathC + \"/testX\")\n",
    "Y_TestC=load_model(pathC + \"/testY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for subtask A: 0.7635732850292878\n",
      "Macro F1 score for subtask B: 0.6985407066052227\n",
      "Macro F1 score for subtask C: 0.6384019466033122\n"
     ]
    }
   ],
   "source": [
    "log_A=load_model(pathA + \"/log\")\n",
    "log_B=load_model(pathB + \"/log\")\n",
    "log_C=load_model(pathC + \"/log\")\n",
    "\n",
    "Y_Pred_A=log_A.predict(X_TestA)\n",
    "Y_Pred_B=log_B.predict(X_TestB)\n",
    "Y_Pred_C=log_C.predict(X_TestC)\n",
    "\n",
    "print(\"Macro F1 score for subtask A:\",f1_score(Y_TestA,Y_Pred_A,average='macro'))\n",
    "print(\"Macro F1 score for subtask B:\",f1_score(Y_TestB,Y_Pred_B,average='macro'))\n",
    "print(\"Macro F1 score for subtask C:\",f1_score(Y_TestC,Y_Pred_C,average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for subtask A: 0.7412340527962769\n",
      "Macro F1 score for subtask B: 0.6563738538893819\n",
      "Macro F1 score for subtask C: 0.6258757297748123\n"
     ]
    }
   ],
   "source": [
    "svm_A=load_model(pathA + \"/svm\")\n",
    "svm_B=load_model(pathB + \"/svm\")\n",
    "svm_C=load_model(pathC + \"/svm\")\n",
    "\n",
    "Y_Pred_A=svm_A.predict(X_TestA)\n",
    "Y_Pred_B=svm_B.predict(X_TestB)\n",
    "Y_Pred_C=svm_C.predict(X_TestC)\n",
    "\n",
    "print(\"Macro F1 score for subtask A:\",f1_score(Y_TestA,Y_Pred_A,average='macro'))\n",
    "print(\"Macro F1 score for subtask B:\",f1_score(Y_TestB,Y_Pred_B,average='macro'))\n",
    "print(\"Macro F1 score for subtask C:\",f1_score(Y_TestC,Y_Pred_C,average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score for subtask A: 0.7106758373205742\n",
      "Macro F1 score for subtask B: 0.4967948717948718\n",
      "Macro F1 score for subtask C: 0.5041338582677165\n"
     ]
    }
   ],
   "source": [
    "rf_A=load_model(pathA + \"/rf\")\n",
    "rf_B=load_model(pathB + \"/rf\")\n",
    "rf_C=load_model(pathC + \"/rf\")\n",
    "\n",
    "Y_Pred_A=rf_A.predict(X_TestA)\n",
    "Y_Pred_B=rf_B.predict(X_TestB)\n",
    "Y_Pred_C=rf_C.predict(X_TestC)\n",
    "\n",
    "print(\"Macro F1 score for subtask A:\",f1_score(Y_TestA,Y_Pred_A,average='macro'))\n",
    "print(\"Macro F1 score for subtask B:\",f1_score(Y_TestB,Y_Pred_B,average='macro'))\n",
    "print(\"Macro F1 score for subtask C:\",f1_score(Y_TestC,Y_Pred_C,average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
