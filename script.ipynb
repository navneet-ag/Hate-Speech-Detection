{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "#Word2Vec\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteratureReport.pdf\n",
      "ml.ipynb\n",
      "OLID\n",
      "README.md\n",
      "roberta.large\n",
      "script.ipynb\n",
      "second.ipynb\n",
      "SOLID\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tweet'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv():\n",
    "    train = pd.read_csv('OLID/olid-training-v1.0.tsv',delimiter='\\t',skiprows=1,header=None)\n",
    "#     train.drop([3], axis = 1,inplace=True)\n",
    "#     train.drop([4], axis = 1,inplace=True)\n",
    "    train[2] = np.where(train[2]==\"NOT\",0,1)\n",
    "    \n",
    "    testlabel = pd.read_csv('OLID/labels-levela.csv',delimiter=',',header=None,dtype=str)\n",
    "    testlabel.drop([0], axis = 1,inplace=True)\n",
    "    testlabel.rename(columns={1: 2},inplace=True)\n",
    "    \n",
    "    testdata = pd.read_csv('OLID/testset-levela.tsv',delimiter='\\t')\n",
    "    print(testdata.columns) \n",
    "    \n",
    "    test = pd.concat([testdata,testlabel], axis=1)\n",
    "    test[2] = np.where(test[2]==\"NOT\",0,1)\n",
    "    test.rename(columns={'id':0,'tweet':1},inplace=True)\n",
    "    \n",
    "    return train,test\n",
    "train,test=loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "      <td>1</td>\n",
       "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[constitutionday, is, revered, by, conservativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[foxnews, nra, maga, potus, trump, 2ndamendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>#Watching #Boomer getting the news that she is...</td>\n",
       "      <td>0</td>\n",
       "      <td>[watching, boomer, getting, the, news, that, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nopasaran, unity, demo, to, oppose, the, farr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>73439</td>\n",
       "      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n",
       "      <td>1</td>\n",
       "      <td>[despicabledems, lie, again, about, rifle, dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>25657</td>\n",
       "      <td>#MeetTheSpeakers ðŸ™Œ @USER will present in our e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[meetthespeakers, user, will, present, in, our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>67018</td>\n",
       "      <td>3 people just unfollowed me for talking about ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, people, just, unfollowed, me, for, talking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>50665</td>\n",
       "      <td>#WednesdayWisdom Antifa calls the right fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>[wednesdaywisdom, antifa, call, the, right, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>24583</td>\n",
       "      <td>#Kavanaugh typical #liberals , #Democrats URL</td>\n",
       "      <td>0</td>\n",
       "      <td>[kavanaugh, typical, liberal, democrat, url]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1  2  \\\n",
       "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...  1   \n",
       "1    27014  #ConstitutionDay is revered by Conservatives, ...  0   \n",
       "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...  0   \n",
       "3    13876  #Watching #Boomer getting the news that she is...  0   \n",
       "4    60133  #NoPasaran: Unity demo to oppose the far-right...  1   \n",
       "..     ...                                                ... ..   \n",
       "855  73439  #DespicableDems lie again about rifles. Dem Di...  1   \n",
       "856  25657  #MeetTheSpeakers ðŸ™Œ @USER will present in our e...  0   \n",
       "857  67018  3 people just unfollowed me for talking about ...  1   \n",
       "858  50665  #WednesdayWisdom Antifa calls the right fascis...  0   \n",
       "859  24583      #Kavanaugh typical #liberals , #Democrats URL  0   \n",
       "\n",
       "                                                 tweet  \n",
       "0    [whoisq, wherestheserver, dumpnike, declasfisa...  \n",
       "1    [constitutionday, is, revered, by, conservativ...  \n",
       "2    [foxnews, nra, maga, potus, trump, 2ndamendmen...  \n",
       "3    [watching, boomer, getting, the, news, that, s...  \n",
       "4    [nopasaran, unity, demo, to, oppose, the, farr...  \n",
       "..                                                 ...  \n",
       "855  [despicabledems, lie, again, about, rifle, dem...  \n",
       "856  [meetthespeakers, user, will, present, in, our...  \n",
       "857  [3, people, just, unfollowed, me, for, talking...  \n",
       "858  [wednesdaywisdom, antifa, call, the, right, fa...  \n",
       "859       [kavanaugh, typical, liberal, democrat, url]  \n",
       "\n",
       "[860 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def processdf(df):\n",
    "    df['tweet'] = df[1].apply(utils_preprocess_text)\n",
    "    df['tweet'] = df['tweet'].apply(word_tokenize)\n",
    "processdf(train)\n",
    "processdf(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13240, 15) (860, 15)\n",
      "(13240,) (860,)\n",
      "(20438, 300)\n"
     ]
    }
   ],
   "source": [
    "def getEmbeddings(train,test):\n",
    "    corpus = pd.concat([train['tweet'],test['tweet']])\n",
    "    # print(len(train),len(test),len(corpus))\n",
    "    # vecmodel = gensim.models.word2vec.Word2Vec(corpus, size=300,window=8, min_count=1, sg=1, iter=30)\n",
    "    tk=tf.keras.preprocessing.text.Tokenizer(lower=True,split=' ',oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tk.fit_on_texts(corpus)\n",
    "\n",
    "    train['tweet_id'] = tk.texts_to_sequences(train['tweet'])\n",
    "    test['tweet_id'] = tk.texts_to_sequences(test['tweet'])\n",
    "\n",
    "    Xtrain = tf.keras.preprocessing.sequence.pad_sequences(train['tweet_id'], maxlen=15,padding=\"post\", truncating=\"post\",value=0)\n",
    "    Xtest = tf.keras.preprocessing.sequence.pad_sequences(test['tweet_id'], maxlen=15,padding=\"post\", truncating=\"post\",value=0)\n",
    "\n",
    "    ytrain = train[2]\n",
    "    ytest = test[2]\n",
    "\n",
    "    embed = np.zeros((len(tk.word_index)+1, 300))\n",
    "    for word,idx in tk.word_index.items():\n",
    "        try:\n",
    "            embed[idx]=vecmodel.wv[word]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    print(Xtrain.shape,Xtest.shape)\n",
    "    print(ytrain.shape,ytest.shape)\n",
    "    print(embed.shape)\n",
    "    \n",
    "    return Xtrain,Xtest,ytrain,ytest,embed\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest,embed = getEmbeddings(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20438, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20436"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embed.shape)\n",
    "np.sum(embed.sum(axis=1) != 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 15, 300)      6131400     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 300, 15)      0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 300, 15)      240         permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention (Permute)             (None, 15, 300)      0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 15, 300)      0           embedding_16[0][0]               \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 15, 30)       37920       multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 30)           5520        bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 64)           1984        bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1)            65          dense_35[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,177,129\n",
      "Trainable params: 45,729\n",
      "Non-trainable params: 6,131,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = keras.layers.Permute((2,1))(inputs)\n",
    "    x = keras.layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = keras.layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = keras.layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "x_in = keras.layers.Input(shape=(15,))\n",
    "x = Embedding(input_dim=embed.shape[0],output_dim=embed.shape[1],weights=[embed],input_length=15,trainable=False)(x_in)\n",
    "x = attention_layer(x, neurons=15)\n",
    "x = Bidirectional(keras.layers.LSTM(units=15, dropout=0.2, return_sequences=True))(x)\n",
    "x = Bidirectional(keras.layers.LSTM(units=15, dropout=0.2))(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "y_out = Dense(1, activation='softmax')(x)\n",
    "model = keras.models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "training = model.fit(x=Xtrain, y=ytrain, batch_size=256,epochs=10, shuffle=True, verbose=0, validation_split=0.3)\n",
    "\n",
    "testPred = model.predict(Xtest)\n",
    "trainPred = model.predict(Xtrain)\n",
    "\n",
    "# print(mean_squared_error(testPred, ytest,squared=False))\n",
    "# print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "# print(mean_absolute_error(testPred, ytest))\n",
    "# print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
