{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "#Word2Vec\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteratureReport.pdf\n",
      "main.ipynb\n",
      "ml.ipynb\n",
      "OLID\n",
      "README.md\n",
      "roberta.large\n",
      "script.ipynb\n",
      "second.ipynb\n",
      "SOLID\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tweet'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and rename columns, load all aqi data but specify metro data name\n",
    "def loadcsv():\n",
    "    train = pd.read_csv('OLID/olid-training-v1.0.tsv',delimiter='\\t',skiprows=1,header=None)\n",
    "#     train.drop([3], axis = 1,inplace=True)\n",
    "#     train.drop([4], axis = 1,inplace=True)\n",
    "    train[2] = np.where(train[2]==\"NOT\",0,1)\n",
    "    \n",
    "    testlabel = pd.read_csv('OLID/labels-levela.csv',delimiter=',',header=None,dtype=str)\n",
    "    testlabel.drop([0], axis = 1,inplace=True)\n",
    "    testlabel.rename(columns={1: 2},inplace=True)\n",
    "    \n",
    "    testdata = pd.read_csv('OLID/testset-levela.tsv',delimiter='\\t')\n",
    "    print(testdata.columns) \n",
    "    \n",
    "    test = pd.concat([testdata,testlabel], axis=1)\n",
    "    test[2] = np.where(test[2]==\"NOT\",0,1)\n",
    "    test.rename(columns={'id':0,'tweet':1},inplace=True)\n",
    "    \n",
    "    return train,test\n",
    "train,test=loadcsv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "      <td>1</td>\n",
       "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[constitutionday, is, revered, by, conservativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[foxnews, nra, maga, potus, trump, 2ndamendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>#Watching #Boomer getting the news that she is...</td>\n",
       "      <td>0</td>\n",
       "      <td>[watching, boomer, getting, the, news, that, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nopasaran, unity, demo, to, oppose, the, farr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>73439</td>\n",
       "      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n",
       "      <td>1</td>\n",
       "      <td>[despicabledems, lie, again, about, rifle, dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>25657</td>\n",
       "      <td>#MeetTheSpeakers ðŸ™Œ @USER will present in our e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[meetthespeakers, user, will, present, in, our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>67018</td>\n",
       "      <td>3 people just unfollowed me for talking about ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, people, just, unfollowed, me, for, talking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>50665</td>\n",
       "      <td>#WednesdayWisdom Antifa calls the right fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>[wednesdaywisdom, antifa, call, the, right, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>24583</td>\n",
       "      <td>#Kavanaugh typical #liberals , #Democrats URL</td>\n",
       "      <td>0</td>\n",
       "      <td>[kavanaugh, typical, liberal, democrat, url]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1  2  \\\n",
       "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...  1   \n",
       "1    27014  #ConstitutionDay is revered by Conservatives, ...  0   \n",
       "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...  0   \n",
       "3    13876  #Watching #Boomer getting the news that she is...  0   \n",
       "4    60133  #NoPasaran: Unity demo to oppose the far-right...  1   \n",
       "..     ...                                                ... ..   \n",
       "855  73439  #DespicableDems lie again about rifles. Dem Di...  1   \n",
       "856  25657  #MeetTheSpeakers ðŸ™Œ @USER will present in our e...  0   \n",
       "857  67018  3 people just unfollowed me for talking about ...  1   \n",
       "858  50665  #WednesdayWisdom Antifa calls the right fascis...  0   \n",
       "859  24583      #Kavanaugh typical #liberals , #Democrats URL  0   \n",
       "\n",
       "                                                 tweet  \n",
       "0    [whoisq, wherestheserver, dumpnike, declasfisa...  \n",
       "1    [constitutionday, is, revered, by, conservativ...  \n",
       "2    [foxnews, nra, maga, potus, trump, 2ndamendmen...  \n",
       "3    [watching, boomer, getting, the, news, that, s...  \n",
       "4    [nopasaran, unity, demo, to, oppose, the, farr...  \n",
       "..                                                 ...  \n",
       "855  [despicabledems, lie, again, about, rifle, dem...  \n",
       "856  [meetthespeakers, user, will, present, in, our...  \n",
       "857  [3, people, just, unfollowed, me, for, talking...  \n",
       "858  [wednesdaywisdom, antifa, call, the, right, fa...  \n",
       "859       [kavanaugh, typical, liberal, democrat, url]  \n",
       "\n",
       "[860 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def processdf(df):\n",
    "    df['tweet'] = df[1].apply(utils_preprocess_text)\n",
    "    df['tweet'] = df['tweet'].apply(word_tokenize)\n",
    "processdf(train)\n",
    "processdf(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.28740916e-02  4.39802200e-01 -4.63868052e-01 -1.70535102e-01\n",
      " -4.23907429e-01  3.06187987e-01  1.12206675e-01 -1.81175873e-01\n",
      "  2.23047078e-01  3.91755819e-01  4.42473404e-02  4.26364802e-02\n",
      "  4.27939594e-01 -1.74144566e-01  7.30657652e-02 -2.58322299e-01\n",
      " -2.65732318e-01 -2.86689013e-01  3.62401247e-01 -1.90714255e-01\n",
      "  8.62595066e-02  4.37412709e-01 -1.75225988e-01  1.01692580e-01\n",
      "  3.92160825e-02 -1.35381728e-01 -3.28089505e-01  2.71916300e-01\n",
      "  2.28904709e-01  1.47348158e-02  2.32596487e-01 -2.26412583e-02\n",
      "  7.68175796e-02 -3.07510704e-01  2.74040550e-01  2.47283921e-01\n",
      "  3.43205094e-01  1.22091353e-01 -3.36508870e-01  9.48349237e-02\n",
      " -1.57745302e-01 -2.60672003e-01 -1.51178285e-01 -3.72529119e-01\n",
      "  1.78581104e-01 -1.85131598e-02  3.40127438e-01 -3.26425172e-02\n",
      " -4.16175753e-01  3.01316917e-01  8.88589621e-02 -1.13395274e-01\n",
      " -4.35705990e-01 -5.92932642e-01 -3.26424211e-01  1.96147189e-01\n",
      " -5.31980433e-02  1.73851311e-01 -1.21057726e-01 -1.78394094e-01\n",
      " -1.06220655e-01 -1.89827140e-02  3.23785245e-01  4.21556830e-01\n",
      "  2.65448451e-01 -3.04605722e-01  3.18112731e-01 -2.51681864e-01\n",
      " -3.15712541e-01 -3.32914025e-01 -2.88763225e-01 -1.83700085e-01\n",
      " -5.86812105e-03  2.32149363e-01  2.21557662e-01 -3.36914092e-01\n",
      " -1.08178118e-02 -1.90604061e-01 -3.72377098e-01  2.26129621e-01\n",
      "  2.93112010e-01  2.61304587e-01 -4.83390123e-01  2.99571216e-01\n",
      " -2.43453398e-01 -1.30927026e-01  1.95522845e-01 -4.45584118e-01\n",
      " -7.38833547e-02 -4.97576952e-01 -4.05423075e-01  8.17827657e-02\n",
      " -4.64384817e-02 -5.30520558e-01 -2.86142975e-01  3.43929589e-01\n",
      "  3.47208917e-01  3.40541422e-01  5.55302426e-02 -2.59900212e-01\n",
      " -1.02931581e-01  1.12094603e-01  2.90626466e-01  2.96790898e-01\n",
      " -1.35035723e-01  3.58194202e-01 -6.19755566e-01  3.03608805e-01\n",
      " -2.80774593e-01 -4.01036501e-01 -1.88521028e-01 -1.54120490e-01\n",
      " -1.48172081e-01 -2.73730606e-01  7.14814425e-01  1.78939179e-01\n",
      "  2.46828422e-01  1.17407516e-02 -2.13644028e-01  4.82551940e-02\n",
      " -2.65734904e-02 -1.69948429e-01 -2.72752166e-01  1.30606160e-01\n",
      "  7.35000670e-02 -4.03542846e-01 -2.11204976e-01 -4.60385591e-01\n",
      " -2.96599772e-02 -3.31975758e-01  6.41161064e-03 -8.53138790e-02\n",
      "  2.66058207e-01 -1.83871210e-01  1.47901148e-01  1.99410930e-01\n",
      "  5.06234989e-02  4.47566897e-01 -1.29955173e-01  5.78284562e-02\n",
      "  3.38535309e-02  1.94209263e-01  2.06183463e-01  8.77872631e-02\n",
      "  5.26129067e-01  3.54132503e-01 -1.76597059e-01 -2.22801507e-01\n",
      "  7.05211833e-02  3.45154524e-01  1.03411086e-01 -1.05300263e-01\n",
      "  2.20966533e-01  4.22513969e-02 -1.32596716e-01 -4.59231824e-01\n",
      " -1.02279738e-01  2.94008493e-01  4.14437614e-03 -1.51919812e-01\n",
      " -6.66732118e-02  2.25567311e-01  1.80789009e-01 -4.10865635e-01\n",
      "  5.55773377e-01  2.69293100e-01  1.77146167e-01  6.94430619e-02\n",
      " -1.63824588e-01 -1.47852182e-01 -6.86671972e-01 -9.02890489e-02\n",
      "  2.40273088e-01  1.21470757e-01  3.01130921e-01  3.46996278e-01\n",
      "  4.66027483e-02  6.82836100e-02 -2.11945567e-02  3.41678411e-01\n",
      " -1.08136699e-01 -1.49823442e-01 -1.87016875e-02 -2.52953805e-02\n",
      " -2.64318883e-01 -2.85049900e-02  2.86510944e-01  1.31313980e-01\n",
      " -2.12012485e-01 -2.65533984e-01  5.05244397e-02 -2.80809492e-01\n",
      "  8.27431530e-02 -5.64789295e-01 -1.73920940e-03  8.20660964e-02\n",
      "  4.82284158e-01  4.50174630e-01  7.15351254e-02 -9.10314098e-02\n",
      " -1.13717370e-01 -2.10094184e-01 -3.40639167e-02  1.05292328e-01\n",
      "  7.02167973e-02  8.25782865e-03 -6.83031837e-03  1.06011860e-01\n",
      "  2.70542443e-01 -1.61956176e-02  1.58610493e-01 -5.12224808e-02\n",
      " -8.90807882e-02  6.40274510e-02  3.66235912e-01 -5.73229492e-01\n",
      " -2.45395273e-01  7.22170249e-02  4.41358894e-01 -2.86819845e-01\n",
      " -1.14908107e-06  4.08400834e-01 -2.72590797e-02  1.51602313e-01\n",
      " -2.77220011e-01 -2.29856700e-01  1.12910313e-03 -2.21618906e-01\n",
      " -2.43815914e-01 -2.35011101e-01  5.80902249e-02  4.85130101e-02\n",
      "  3.33689600e-01 -2.00594023e-01  1.29190892e-01 -1.93818301e-01\n",
      " -4.10895385e-02  8.86607096e-02  5.93667217e-02  1.78816676e-01\n",
      "  3.01646531e-01 -3.05603415e-01 -1.95847392e-01 -3.84723365e-01\n",
      " -3.60557646e-01  1.70302063e-01 -1.63570672e-01 -2.37826869e-01\n",
      " -4.34327513e-01  2.32939243e-01  2.93779820e-02  4.64652479e-02\n",
      " -4.95540686e-02  3.21418971e-01 -2.36402869e-01 -7.33029127e-01\n",
      " -1.82533264e-01  1.48812616e-02  1.91814408e-01 -2.53105253e-01\n",
      " -3.10322613e-01  3.72195244e-01 -2.23046839e-01  3.15767169e-01\n",
      " -3.13365757e-02 -2.26961672e-01  1.62364617e-01 -1.21504866e-01\n",
      " -2.93723434e-01 -1.76966742e-01 -3.73997808e-01  5.68490662e-02\n",
      "  1.99463498e-02  4.23018515e-01 -1.59546033e-01 -1.27384201e-01\n",
      " -1.84779480e-01  1.36569709e-01 -3.39537740e-01 -1.28849179e-01\n",
      "  1.64953813e-01 -5.11548333e-02  1.30219951e-01 -9.92620438e-02\n",
      "  3.06770027e-01 -4.96780723e-01  6.62815571e-02 -5.06902337e-01\n",
      "  2.60703385e-01  5.23181260e-01 -9.51964483e-02  1.57898977e-01\n",
      " -4.37499620e-02  8.65187496e-02  1.85867906e-01 -3.48502360e-02\n",
      " -1.41771510e-01  9.65178944e-03  1.58754468e-01 -3.65170129e-02]\n",
      "(13240, 15) (860, 15)\n",
      "(13240, 1) (860, 1)\n",
      "(20438, 300)\n"
     ]
    }
   ],
   "source": [
    "def getEmbeddings(train,test):\n",
    "    corpus = pd.concat([train['tweet'],test['tweet']])\n",
    "    # print(len(train),len(test),len(corpus))\n",
    "    vecmodel = gensim.models.word2vec.Word2Vec(corpus, size=300,window=8, min_count=1, sg=1, iter=30)\n",
    "    tk=tf.keras.preprocessing.text.Tokenizer(lower=True,split=' ',oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tk.fit_on_texts(corpus)\n",
    "\n",
    "    train['tweet_id'] = tk.texts_to_sequences(train['tweet'])\n",
    "    test['tweet_id'] = tk.texts_to_sequences(test['tweet'])\n",
    "\n",
    "    Xtrain = tf.keras.preprocessing.sequence.pad_sequences(train['tweet_id'], maxlen=15,padding=\"post\", truncating=\"post\",value=0)\n",
    "    Xtest = tf.keras.preprocessing.sequence.pad_sequences(test['tweet_id'], maxlen=15,padding=\"post\", truncating=\"post\",value=0)\n",
    "\n",
    "    ytrain = np.array(train[2]).reshape((train[2].shape[0],1))\n",
    "    ytest = np.array(test[2]).reshape((test[2].shape[0],1))\n",
    "    \n",
    "    print(vecmodel.wv['she'])\n",
    "    embed = np.zeros((len(tk.word_index)+1, 300))\n",
    "    for word,idx in tk.word_index.items():\n",
    "        try:\n",
    "            embed[idx]=vecmodel.wv[word]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    print(Xtrain.shape,Xtest.shape)\n",
    "    print(ytrain.shape,ytest.shape)\n",
    "    print(embed.shape)\n",
    "    \n",
    "    return Xtrain,Xtest,ytrain,ytest,embed\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest,embed = getEmbeddings(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 15, 300)      6131400     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 300, 15)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 300, 15)      240         permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention (Permute)             (None, 15, 300)      0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 15, 300)      0           embedding_4[0][0]                \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 15, 30)       37920       multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 30)           5520        bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           1984        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            65          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,177,129\n",
      "Trainable params: 45,729\n",
      "Non-trainable params: 6,131,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 10592 samples, validate on 2648 samples\n",
      "Epoch 1/10\n",
      "10592/10592 [==============================] - 9s 874us/step - loss: 0.6553 - accuracy: 0.6590 - val_loss: 0.6341 - val_accuracy: 0.6688\n",
      "Epoch 2/10\n",
      "10592/10592 [==============================] - 6s 601us/step - loss: 0.6336 - accuracy: 0.6674 - val_loss: 0.6272 - val_accuracy: 0.6688\n",
      "Epoch 3/10\n",
      "10592/10592 [==============================] - 7s 641us/step - loss: 0.6172 - accuracy: 0.6676 - val_loss: 0.5931 - val_accuracy: 0.6881\n",
      "Epoch 4/10\n",
      "10592/10592 [==============================] - 6s 603us/step - loss: 0.5889 - accuracy: 0.6866 - val_loss: 0.5715 - val_accuracy: 0.7088\n",
      "Epoch 5/10\n",
      "10592/10592 [==============================] - 6s 607us/step - loss: 0.5739 - accuracy: 0.7034 - val_loss: 0.5641 - val_accuracy: 0.7160\n",
      "Epoch 6/10\n",
      "10592/10592 [==============================] - 7s 666us/step - loss: 0.5654 - accuracy: 0.7107 - val_loss: 0.5574 - val_accuracy: 0.7255\n",
      "Epoch 7/10\n",
      "10592/10592 [==============================] - 6s 608us/step - loss: 0.5617 - accuracy: 0.7147 - val_loss: 0.5539 - val_accuracy: 0.7239\n",
      "Epoch 8/10\n",
      "10592/10592 [==============================] - 6s 608us/step - loss: 0.5557 - accuracy: 0.7244 - val_loss: 0.5546 - val_accuracy: 0.7232\n",
      "Epoch 9/10\n",
      " 1792/10592 [====>.........................] - ETA: 3s - loss: 0.5682 - accuracy: 0.7249"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = keras.layers.Permute((2,1))(inputs)\n",
    "    x = keras.layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = keras.layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = keras.layers.multiply([inputs, x])\n",
    "    return x\n",
    "with tf.device('/CPU:0'):\n",
    "    x_in = keras.layers.Input(shape=(15,))\n",
    "    x = Embedding(input_dim=embed.shape[0],output_dim=embed.shape[1],weights=[embed],input_length=15,trainable=False)(x_in)\n",
    "    x = attention_layer(x, neurons=15)\n",
    "    x = Bidirectional(keras.layers.LSTM(units=15, dropout=0.2, return_sequences=True))(x)\n",
    "    x = Bidirectional(keras.layers.LSTM(units=15, dropout=0.2))(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    y_out = Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.models.Model(x_in, y_out)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    training = model.fit(Xtrain,ytrain,batch_size=256,epochs=10, shuffle=True, verbose=1, validation_split=0.2)\n",
    "\n",
    "    testPred = model.predict(Xtest)\n",
    "    trainPred = model.predict(Xtrain)\n",
    "\n",
    "# print(mean_squared_error(testPred, ytest,squared=False))\n",
    "# print(mean_squared_error(trainPred, ytrain,squared=False))\n",
    "# print(mean_absolute_error(testPred, ytest))\n",
    "# print(mean_absolute_error(trainPred, ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
